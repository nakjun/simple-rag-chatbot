import os
import warnings
import streamlit as st
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from pdf_to_md import convert_pdf_to_markdown
import torch
import tiktoken
import openai

warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning, message="coroutine 'expire_cache' was never awaited")

# OpenAI API í‚¤ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜
@st.cache_resource
def initialize_embedding_model(model_name):
    return OpenAIEmbeddings(model=model_name)

# í† í° ìˆ˜ ê³„ì‚° í•¨ìˆ˜
def num_tokens_from_string(string: str, encoding_name: str) -> int:
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

# PDF ì²˜ë¦¬ ë° ë²¡í„° DB ìƒì„± í•¨ìˆ˜
def process_pdf(pdf_file, embedding_model):
    # PDFë¥¼ Markdownìœ¼ë¡œ ë³€í™˜
    markdown_content = convert_pdf_to_markdown(pdf_file)
    
    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_text(markdown_content)
    
    # í† í° ìˆ˜ ê³„ì‚°
    total_tokens = sum(num_tokens_from_string(text, "cl100k_base") for text in texts)
    
    # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    vectorstore = FAISS.from_texts(texts, embedding_model)
    return vectorstore, total_tokens

# ë¹„ìš© ê³„ì‚° í•¨ìˆ˜
def calculate_embedding_cost(total_tokens, model_name):
    if model_name == "text-embedding-3-small":
        return total_tokens * 0.00002 / 1000  # $0.02 per 1000 tokens
    elif model_name == "text-embedding-3-large":
        return total_tokens * 0.00013 / 1000  # $0.13 per 1000 tokens
    else:
        return 0

# OpenAI ëª¨ë¸ ì´ˆê¸°í™”
@st.cache_resource
def init_openai_model():
    return ChatOpenAI(model="gpt-4o-mini-2024-07-18", temperature=0.001)

# ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
def retrieve_docs(vectordb, query: str, k: int = 5):
    docs_and_scores = vectordb.similarity_search_with_score(query, k=k)
    
    results = []
    for doc, score in docs_and_scores:
        results.append({
            "content": doc.page_content,
            "metadata": doc.metadata,
            "similarity_score": score
        })
    
    return results

# Streamlit ì•± ë©”ì¸ í•¨ìˆ˜
def main():
    st.title("Simple RAG Chatbot")

    with st.sidebar:
        st.title("RAG Chatbot")
        st.markdown("---")
        st.markdown("Developed by [nakjun](https://github.com/nakjun)")
        embedding_option = st.selectbox("ì„ë² ë”© ëª¨ë¸ ì„ íƒ", ("large", "small"))
        uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf")

    if embedding_option == "large":
        embedding_model = initialize_embedding_model("text-embedding-3-large")
        model_name = "text-embedding-3-large"
    else:
        embedding_model = initialize_embedding_model("text-embedding-3-small")
        model_name = "text-embedding-3-small"

    if uploaded_file is not None:
        with st.spinner("PDF íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
            vectordb, total_tokens = process_pdf(uploaded_file, embedding_model)
        st.success("ì—…ë¡œë“œí•œ PDF íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.")

        # ì„ë² ë”© ë¹„ìš© ê³„ì‚°
        embedding_cost = calculate_embedding_cost(total_tokens, model_name)

        # ì‚¬ì´ë“œë°”ì— í† í° ìˆ˜ì™€ ë¹„ìš© í‘œì‹œ
        with st.sidebar:
            st.markdown("---")
            st.subheader("ì„ë² ë”© ì •ë³´")
            st.write(f"ì´ í† í° ìˆ˜: {total_tokens:,} by tiktoken")
            st.write(f"ì˜ˆìƒ ë¹„ìš©: ${embedding_cost:.4f} by openai")

        # ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•  ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"], avatar=("ğŸ§‘â€ğŸ’»" if message["role"] == "user" else "ğŸ¤–")):
                st.markdown(message["content"])

        # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        st.markdown("---")
        if query_text := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."):
            st.session_state.messages.append({"role": "user", "content": query_text})
            with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
                st.markdown(query_text)

            with st.spinner("RAG ì—”ì§„ì„ ì´ìš©í•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
                results = retrieve_docs(vectordb, query_text)

                # ì‚¬ì´ë“œë°”ì— ê²°ê³¼ ì¶œë ¥
                with st.sidebar:
                    st.subheader("ê²€ìƒ‰ëœ ë¬¸ì„œ:")
                    for i, result in enumerate(results, 1):
                        st.markdown(f"**ë¬¸ì„œ {i}**")
                        st.write(f"ë‚´ìš©: {result['content'][:400]}...")
                        st.write(f"ìœ ì‚¬ë„: {result['similarity_score']}")
                        st.markdown("---")

            if results:
                context = "\n\n".join([f"{result['content']}" for result in results])
                prompt = f"""ë‹¤ìŒì€ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤:

                {context}

                ì „ë‹¬í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:

                ì§ˆë¬¸: {query_text}

                ë‹µë³€ ì‹œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:
                1. ì œê³µëœ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
                2. ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ê±°ë‚˜ ê´€ë ¨ì´ ì—†ëŠ” ê²½ìš°, ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•˜ì„¸ìš”.
                3. ì¶”ì¸¡í•˜ì§€ ë§ê³ , í™•ì‹¤í•œ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”.
                4. ë‹µë³€ì€ ë…¼ë¦¬ì ì´ê³  êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
                5. ì „ë¬¸ ìš©ì–´ê°€ ìˆë‹¤ë©´ ê°„ë‹¨íˆ ì„¤ëª…ì„ ë§ë¶™ì´ì„¸ìš”.
                6. ë‹µë³€ì€ í•œê¸€ë¡œ ì‘ì„±í•˜ì„¸ìš”.

                ë‹µë³€:"""

                with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    try:
                        openai_model = init_openai_model()
                        response = openai_model.predict(prompt)
                        
                        # AI ì‘ë‹µ ì¶”ê°€
                        st.session_state.messages.append({"role": "assistant", "content": response})
                        with st.chat_message("assistant", avatar="ğŸ¤–"):
                            st.markdown(response)

                    except Exception as e:
                        st.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            else:
                st.warning("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()